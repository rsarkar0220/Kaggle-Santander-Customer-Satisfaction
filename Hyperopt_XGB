import pandas as pd
import numpy as np
from scipy.stats import spearmanr
import matplotlib.pyplot as plt
import seaborn as sns
from time import time
from sklearn import cross_validation, metrics 
from sklearn.cross_validation import StratifiedKFold, cross_val_score
from sklearn import feature_selection
from sklearn.pipeline import make_pipeline
import xgboost as xgb
from hyperopt import hp, fmin, tpe, STATUS_OK, Trials

raw_train = pd.read_csv("train.csv")
raw_test = pd.read_csv("test.csv")

# Constant and Duplicate Features to be removed
dropped_feats = pd.read_csv("Dropped_vars.csv")

df_target = pd.DataFrame(raw_train["TARGET"])
train_ID = raw_train["ID"]
test_ID = raw_test["ID"]

raw_train.drop(["ID", "TARGET"], axis = 1, inplace = True)
raw_test.drop(["ID"], axis = 1, inplace = True)
df_combo = pd.concat([raw_train, raw_test], axis = 0, ignore_index = True)
feats_orig = df_combo.columns

df_combo = df_combo.drop(dropped_feats["VarName"], axis = 1)

# Outlier in "var3" replaced by the mode.
df_combo = df_combo.replace(-999999, 2) 

df_cor = pd.read_csv("Spearman_feats_to_drop.csv")
vars_to_drop = list(df_cor["b"])

df_combo_red = df_combo.drop(vars_to_drop, axis = 1)
train_red = df_combo_red.loc[:df_target.shape[0]-1,:]
test_red = df_combo_red.loc[df_target.shape[0]:,:]

D_train = xgb.DMatrix(train_red, label = df_target["TARGET"])
label = D_train.get_label()
ratio = float(np.sum(label == 1)) / np.sum(label==0)

# Use early stopping in XGBoost to find the number of estimators for other commonly used hyper-parameters
start = time()
alg = xgb.XGBClassifier(
 learning_rate =0.1,
 n_estimators= 1000,
 max_depth=5,
 min_child_weight=1,
 gamma= 0,
 subsample= 0.8,
 colsample_bytree= 0.8,
 objective= 'binary:logistic',
 nthread=4,
 scale_pos_weight=ratio,
 seed=4242,
 missing = 9999999999)

xgb_param = alg.get_xgb_params()
        
cvresult = xgb.cv(xgb_param, D_train, num_boost_round=alg.get_params()['n_estimators'], nfold=10,
    metrics=['auc'], early_stopping_rounds=50, show_progress=True, show_stdv=False)

# Hyperopt for finding optimal hyper-parameters
start = time()
print(time()- start)
def objective(space):
    
#    print(hyperopt.pyll.stochastic.sample(space))
    clf = xgb.XGBClassifier(n_estimators = cvresult.shape[0],
                            max_depth = space['max_depth'],
                            min_child_weight = space['min_child_weight'],
                            subsample = space['subsample'],
                            colsample_bytree = space['colsample_bytree'],
                            objective = "binary:logistic",
                            learning_rate = 0.1,
                            nthread = 4,
                            seed = 4242,
                            scale_pos_weight = ratio)
    
    
    #ppl = make_pipeline(slctr, clf)                     
    eval_set  = [(local_train, local_target["TARGET"]), (train_holdout, target_holdout["TARGET"])]
    
    clf.fit(local_train, local_target["TARGET"], 
            eval_set=eval_set, eval_metric = 'auc', verbose = False,
            early_stopping_rounds=30)
           
    pred = clf.predict_proba(train_holdout) 
    auc = metrics.roc_auc_score(target_holdout["TARGET"], pred[:,1])
    print("SCORE:", auc)

    return{'loss':1-auc, 'status': STATUS_OK }


space ={
        'max_depth': hp.quniform("max_depth", 3, 7, 1),
        'min_child_weight': hp.quniform('min_child_weight', 1, 10, 0.01),
        'subsample': hp.quniform('subsample', 0.5, 1.0, 0.05),
        'colsample_bytree': hp.quniform ('colsample_bytree', 0.5, 1.0, 0.05)
    }

best = fmin(fn=objective,
            space=space,
            algo=tpe.suggest,
            max_evals=50)

print(best)
print(time() - start)
